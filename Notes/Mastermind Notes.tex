\documentclass[12pt, a4paper]{article}
\author{Aaron Berger, Christopher Chute, Matthew Stone}
\title{Notes from Mastermind Research}
\usepackage[bottom=1.5in, left=1in, right=1in, top=1.1in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\newtheorem{lem}{Lemma}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{footnote}

% \begin{header footer formatting}
\usepackage{fancyhdr}
\setlength{\headheight}{48pt}
\pagestyle{fancyplain}
\lhead{Mastermind Project: \textit{Notes}\\\today{}}
\rhead{AB, CC, MS}
\rfoot{\thepage}
\cfoot{}
% \end{header footer formatting}

% remove ``*'' for numbered theorems
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

% \begin{pretty i-hat and j-hat}
\newcommand*{\ihat}{\hat{\imath}}
\newcommand*{\jhat}{\hat{\jmath}}
\newcommand*{\jwidehat}{\widehat{\jmath}}
% \end{pretty i-hat and j-hat}

% \begin{pretty code or pseudocode}
\usepackage{listings}
\usepackage{courier}
\lstset{
         basicstyle=\footnotesize\ttfamily,
%        numbers=left,               
         numberstyle=\tiny,          
%        stepnumber=2,              
         numbersep=5pt,              
         tabsize=4,                 
         extendedchars=true,         
         breaklines=true,            
         keywordstyle=\color{red},
         frame=tblr,% Set the borders, tblr == (top, bottom, left, right)
%        keywordstyle=[1]\textbf,    
%        keywordstyle=[2]\textbf,   
%        keywordstyle=[3]\textbf,   
%        keywordstyle=[4]\textbf,  
         stringstyle=\color{white}\ttfamily,
         showspaces=false,          
         showtabs=false,           
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
%        backgroundcolor=\color{red},
         showstringspaces=false            
 }
\lstloadlanguages{C}
% Example Usage:
% \lstinputlisting{pseudocode.txt}
% \end{pretty code or pseudocode}

% ********************************* END OF PREAMBLE ***********************************

\begin{document}
\section*{Mastermind with No Repeats: Lower Bound for Guessing Strategies}
	\begin{enumerate}
	\item\textit{Game.}
	In this version of Mastermind, there are $n$ spots and $n$ colors.
	All guesses and solutions are permutations.
	
	\item \textit{Trivial Lower Bound.}
	We have $n!$ possible hidden vectors, and each query has $n$ possible
	responses.\footnote{The possible responses are $0, 1, \ldots, n-2, n$, since it is
	impossible to get $n-1$ black hits with no repeats and $n=k$.}
	Since we must distinguish between every pair of vectors, we must ask at least
		\begin{equation*}
		\log_{n}(n!)\sim \left(n-\frac{n}{\ln(n)}\right)
		\end{equation*}

	\item\textit{Buckets.}
	In order to beat the trivial lower bound, we use a more accurate picture
	of the distribution of the possible solutions after feedback from a given guess.
	For any guess, we can split the possible solutions up into groups, which
	we will call \textit{buckets}; one bucket for each possible response given by
	the codemaker. For any guess we make, the number of possible solutions that give
	the response ``$k$ (black) hits'' is the number of permutations with $k$ fixed
	points with respect to our guess vector.
	\begin{center}
	\includegraphics[keepaspectratio=true, width=0.7\textwidth]{buckets}
	\end{center}
	The size of a bucket of possible
	solutions with $k$ hits can be counted easily: we choose the $k$ colors that are
	fixed points (wrt our guess vector), then we permute the remaining $n-k$ colors
	without any fixed points, giving $D(n - k)$, the derangements on an $(n-k)$-element
	vector:
		\begin{equation*}
		\binom{n}{k}D(n-k)
		\end{equation*}
	This tells us the initial size of each bucket, and as more guesses are
	made, the size of each bucket can only decrease, no matter which guess is
	considered.
	
	\item\textit{Notation.}
	Let $S_{n}(t)$ be the minimum number of possible solutions remaining
	after $t$ guesses for any guessing strategy. We compute this lower bound by using
	a set of guesses which are guaranteed to eliminate at least as many solutions
	as any other guess we could make. The worst-case scenario for this set of guesses
	therefore provides a lower bound for the number of guesses required by any
	guessing strategy to guarantee finding the hidden vector.
	
	\item\textbf{Recurrence Relation on Number of Possible Solutions after $t$ Turns:}
		\begin{enumerate}
		\item We will inductively prove a lower bound, $S_{n}(t)$. By the
		discussion above,
		we know the maximum size of each bucket containing possible solutions.
		The best possible guess distributes the remaining solutions as evenly as
		possible so that the bucket containing the most remaining solutions is as
		small as possible. This best-possible distribution will fill some number of
		smaller buckets entirely and split the remaining solutions evenly
		into some number $x$ larger buckets. That is, there are $x$ incompletely
		filled buckets.
		
		\item At the beginning of the $t$-th turn, we have at least $S_{n}(t-1)$
		possible
		solutions remaining. As we showed before, the size of each bucket is bounded
		by its original size of $\binom{n}{k}D(n-k)$. Thus the number of solutions
		remaining $(t-1)$-th turn relates to the value of $S_{n}(t)$ as follows:
		
		Let $B_i(t)$ be the exact number of possible solutions remaining in the
		bucket holding possible solutions with $i$ fixed points during the
		$t$-th turn (after submitting the guess vector, but before feedback). So
		\begin{align*}
		S_{n}(t-1) & = \sum_{i = 0}^{n}B_i(t)\\
		& = \sum_{i=0}^{x-1}B_i(t) + \sum_{i=x}^{n}B_i(t)\\
		& \le \sum_{i = 0}^{x-1}S_n(t) + \sum_{i=x}^{n}\binom{n}{i}D(n-i)\quad
		\text{for optimal $x$}\\
		& = x\cdot S_{n}(t) + \sum_{i = x}^{n}\binom{n}{i}D(n-i)
		\end{align*}
		Once we know $S_{n}(t)$, picking $x$ too large results in some smaller boxes
		overfilled. Picking $x$ too small results in some larger boxes overfilled. So
		changing $x$ to be something other than optimal always increases the RHS
		of this equation. Thus we have
		\begin{align*}
		S_{n}(t-1) & \le x\cdot S_{n}(t) + \frac{n!}{x!}\quad\forall x
		\end{align*}
		Where $t$ is the number of turns taken, $S_{n}(t)$ is the number of possible
		solutions remaining after the $t$-th turn, and $D(k)$ is the derangements
		on a set of size $k$.
		\end{enumerate}
		
		\item\textit{Claim.}
			\begin{equation*}
			\sum_{i=x}^n\binom{n}{i}D(n-i) \le \frac{n!}{x!}
			\end{equation*}
			This inequality leads to our bound on $S_{n}(t)$ given
			below.\footnote{A combinatorial argument can be made as follows: The LHS
			denotes the number of permutations of an $n$-element vector which have
			at least $x$ fixed points. Suppose we choose $x$ fixed points and
			simply permute the rest of the vector. This gives $\binom{n}{x}(n-x)!
			=\frac{n!}{x!}$ possible permutations. Clearly this includes all
			vectors with at least $x$ fixed points (and overcounts by some
			margin), so the inequality holds.}
			\begin{proof}
				\begin{align*}
				\sum_{i=x}^n\binom{n}{i}D(n-i) & =
				\frac{n!}{x!}\sum_{i=x}^{n}\frac{x!}{i!(n-i)!}D(n-i)\\
				& \qquad\text{we use the fact that }\frac{x!}{i!}\le\frac{1}{(i-x)!}\\
				& \le \frac{n!}{x!}\sum_{i=x}^{n}\frac{1}{(i-x)!(n-i)!}D(n-i)\\
				& \qquad\text{let $k=i-x$}\\
				& = \frac{n!}{x!}\sum_{k=0}^{n-x}\frac{1}{k!(n-x-k)!}D(n-x-k)\\
				& = \frac{n!}{x!(n-x)!}\sum_{k=0}^{n-x}\binom{n-x}{k}D(n-x-k)\\
				& \qquad\text{note the summation counts all possible permutations}\\
				& = \frac{n!}{x!(n-x)!}(n-x)!\\
				& = \frac{n!}{x!}				
				\end{align*}
			\end{proof}		
		\begin{enumerate}
		\item Plug the above result into the inequality to get
			\begin{equation*}
			S_{n}(t-1) \le x\cdot S_{n}(t) + \frac{n!}{x!}
			\end{equation*}
		So we have
			\begin{equation*}
			S_{n}(t)\ge \frac{1}{x}\left(S_{n}(t-1)-\frac{n!}{x!}\right)
			\end{equation*}
		\end{enumerate}
	\end{enumerate}
	\clearpage
		\begin{theorem} For all $n$, and for any constant $C_n$, we have
			\begin{equation*}
			\frac{S_{n}(t)}{n!}\ge \frac{C_{n}! - (H_{C_{n}+t} - H_{C_{n}})}{(C_n+t)!}
			\end{equation*}
			\begin{equation*}
			\text{where }\quad0\le t\le n-C_{n}
			\end{equation*}
		\end{theorem}
	Here we use $H_n$ to denote the $n$-th harmonic number.\footnote{$H_n=\sum_{i = 1}
	^n\frac{1}{i}$}
		\begin{proof}
		While this equation holds for all $x$, to achieve a desirable bound, we will
		let $x=t+C_{n}$ for any positive constant $C_{n}$. We have
		\begin{enumerate}[label=]
		\item\textit{Base Case.} When $t=0$, we have $S_n(0)= n!$ and
			\begin{equation*}
			\frac{S_{n}(0)}{n!}\ge \frac{C_{n}! - (H_{C_{n}}-H_{C_{n}})}{C_{n}!} = 1
			\end{equation*}
			
		\item \textit{Inductive Step.} Assume the induction hypothesis holds for $t-1$.
			\begin{align*}
			\frac{S_{n}(t)}{n!}
			& \ge \frac{1}{x}\left(\frac{S_{n}(t-1)}{n!}-\frac{1}{x!}\right)
			\end{align*}
		Let $x=C_{n}+t$.
			\begin{align*}
			\frac{S_{n}(t)}{n!}
			& \ge \frac{1}{C_{n}+t}\left(\frac{S_{n}(t-1)}{n!}
			-\frac{1}{(C_{n}+t)!}\right)
			\end{align*}
		Inductively,
			\begin{align*}
			\frac{S_{n}(t)}{n!}
			& \ge \frac{1}{C_{n}+t}\left(\frac{C_{n}!-(H_{C_{n}+t} - H_{C_{n}})}
			{(C_{n}+t-1)!} - \frac{1}{(C_{n}+t)!}\right)\\
			& \ge \left(\frac{C_{n}! - (H_{C_{n}+t-1} - H_{C_{n}}) - \frac{1}{C_{n}+t}}
			{(C_{n}+t)!}\right)\\
			& \ge \left(\frac{C_{n}! - (H_{C_{n}+t} - H_{C_{n}})}{(C_{n}+t)!}\right)
			\end{align*}
			Then
			\begin{align*}
			S_{n}(n-C_{n}) & \ge n!\left(\frac{C_{n}! - (H_n - H_{C_{n}})}{n!}\right)\\
			& \ge C_{n}! - (H_n-H_{C_{n}})
			\end{align*}
		Thus the inequality holds for all $n$.
		\end{enumerate}
		\end{proof}
		We choose $C_{n}$ by the following reasoning: we want to achieve a lower bound
		of $n-C_{n}$ turns. Thus, we want $S_n(n-C_{n})>1$, which follows if
		\begin{equation*}
		C_{n}! - (H_n - H_{C_{n}}) > 1
		\end{equation*}
		For $C_{n}=\log\log n$, this is $\ge 1$ for $n\ge e^{198}$.\\
		For $C_{n}=\log n$, this is true for $n\ge 10$.
		
 	In conclusion, when $n$ is sufficiently large, the minimum number of remaining
	possible solutions after $n - \log\log n$ guesses is at least
		\begin{align*}
		S_n(n-\log\log n) & \ge (\log\log n)! - (H_n - H_{\log\log n})\\
		& > 1
		\end{align*}
	Thus there is no strategy that can reduce the set of possible solutions to
	one, and guarantee finding the codemaker's hidden vector, in $n-\log\log n$
	turns or fewer.

\clearpage
\section*{Lower Bound on Guessing Effectiveness for Permutation Game}

	\begin{theorem} In Mastermind with no repeats with $n$ spots and $k$ colors,
	for any set of possible remaining solutions, there exists a
	guess vector for which any response will eliminate at least $1/nk$ of the
	remaining solutions.
	\end{theorem}
	\begin{enumerate}
	\item\textit{Notation.}
		\begin{enumerate}[label=\roman*.]
		\item\textit{Basics.} Let $n$ be the number of spots in the hidden vector.
		Let $k$ be the number of colors available. Since we allow no repeats in this
		variant of Mastermind, we have $k\ge n$. We will use single-count responses,
		which are the number of spots in which our guess vector and the hidden vector
		have the same color in that spot.
		\item\textit{Remaining Solution Set.} As the game proceeds, we maintain a set
		$S$ that contains all possible solution vectors that haven't been ruled out
		by a previous guess and response.
		\item\textit{Bucket.} As defined in the previous section, a guess splits
		the remaining possible solutions into ``buckets,'' where a bucket $B_i$
		contains all the remaining possible solutions with $i$ fixed points with
		respect to the guess vector.
		\item\textit{Sub-Bucket.} Within a bucket, we will divide the vectors
		based on a chosen component, say the first element of each vector.
		Then there will be $k$ sub-buckets $D_i$, where $D_1$ contains all vectors
		from the bucket with a $1$ in the first spot, $D_2$ contains all those
		with a $2$ in the first spot, and so on.
		\end{enumerate}

	\item
		\begin{proof}
		We give a proof by induction on $n$.
			\begin{enumerate}[label=]
			\item\textit{Base Case.} When $n=1$, we have a single-element vector.
			So there are at most $k$ possible solutions remaining at any step, thus
			$|S|\le k$. If we guess one of the remaining solutions $v\in S$, this
			will eliminate $1$ solution ($v$ itself) if the response is $0$
			and $|S|-1$ solutions if the response is $1$. So we're guaranteed to
			eliminate at least $1/|S|\ge 1/(nk)$ solutions.
			
			\item\textit{Inductive Step.} Assume that for $n-1$ spots and any number
			of colors $\widehat{k}$ (where $n\ge 2$), we are guaranteed that
			there is a guess that eliminates a fraction at least $1/(n\widehat{k})$
			of the solutions.
			
			Consider a guess vector $\sigma=\{\sigma_1, \sigma_2, \ldots, \sigma_n\}$.
			We will analyze the number of solutions
			eliminated from $S$ by guessing $\sigma$ in the worst-case. We divide $S$
			into $n$ buckets $B_1, B_2, \ldots, B_n$, where $B_i$ is as defined above
			($i$ fixed points with respect to $\sigma$). Consider the response $j$
			that eliminates as few possible solutions as possible from $S$,
			so $|B_j| = \max\{|B_1|, |B_2|, \ldots, |B_n|\}$.
			
			Suppose that
				\begin{equation*}
				|B_j|\le\frac{nk - 1}{nk}\cdot|S|
				\end{equation*}
			Then the response $j$ would eliminate at least $|S|/(nk)$ solutions from
			$S$, and since $j$ eliminates the fewest possible solutions of any
			response, all responses to $\sigma$ eliminate a fraction at least
			$1/(nk)$ of the possible solutions in $S$.
			
			Otherwise we have
				\begin{equation*}
				|B_j|>\frac{nk - 1}{nk}\cdot|S|
				\end{equation*}
			\textit{Case 1.} If $j>0$, the total number of hits among all vectors in
			$B_j$ is $j\cdot |B_j|$. These hits are divided up among $n$ spots,
			therefore by the pigeonhole principle, at least one position
			must have at least
				\begin{align*}
				\frac{j}{n}\cdot|B_j|
				\end{align*}
			Vectors that match the guess at that spot. WLOG, we assume this is
			position one. We now divide the bucket $B_j$ into sub-buckets $D_i$,
			where permutation $s\in B_j$ is put into sub-bucket $D_{s_1}$. From above,
			we have 
			\begin{equation*}
			|D_{\sigma_1}|\ge \frac{j}{n}\cdot|B_j|
			\end{equation*}
			We have two sub-cases:
				\begin{enumerate}[label=\roman*.]
				\item $|D_{\sigma_1}|\le \dfrac{n-1}{n}\cdot|B_j|$:\\
				Then we apply the pigeonhole principle on the remaining solutions
				in $B_j$ to find the second-largest sub-bucket:
				By our assumption, there are at least $|B_j|/n$ solutions remaining
				in $B_j$, and there are $k-1$ remaining sub-buckets $D_i$. By the
				pigeonhole principle, there exists a $D_\alpha$ such that 
				\begin{equation*}
				|D_\alpha| \ge \frac{1}{n}\cdot\frac{1}{k-1}\cdot|B_j|
				\end{equation*}
				If $\alpha$ is an element of $\sigma$, then WLOG
    			let the color be $\sigma_2$. Then in this case we consider the
				guess vector
    			$\overline{\sigma}=\{\sigma_2, \sigma_1, \sigma_3, \ldots, \sigma_n\}$.
    			Otherwise, consider the guess vector
    			$\overline{\sigma}=\{\alpha, \sigma_2, \sigma_3, \ldots, \sigma_n\}$.
				Consider the buckets $C_i$ formed from the guess vector
				$\overline{\sigma}$. In either case, the vectors in $D_{\sigma_1}$ will
				have at least one fewer hit with $\overline{\sigma}$ than with
				$\sigma$, and the guess vectors in $D_{\alpha}$ will have at least
				one more hit. Therefore, any response to $\overline{\sigma}$ which
				corresponds to some vectors in $D_{\sigma_1}$ corresponds to none
				of the vectors in $D_{\alpha}$ and vice versa. So we know that any
				response to $\overline{\sigma}$ will entirely eliminate at least one
				of those two sets. From above, we know
					\begin{align*}
					|D_{\sigma_1}| & \ge \frac{j}{n}\cdot|B_j|\\
					& \ge \frac{j}{n}\cdot\frac{nk-1}{nk}\cdot|S|\\
					& \ge \frac{1}{n}\cdot\frac{n}{nk}\cdot|S|\\
					& = \frac{1}{nk}\cdot|S|
					\end{align*}
				And
					\begin{align*}
					|D_\alpha| & \ge\frac{1}{n}\cdot\frac{1}{k-1}\cdot|B_j|\\
					& \ge \frac{1}{nk-n}\cdot\frac{nk-1}{nk}\cdot|S|\\
					& \ge \frac{1}{nk}\cdot|S|
					\end{align*}
				Since $\overline{\sigma}$ will always eliminate at least one of
				these two sets, we also have that $\overline{\sigma}$ will eliminate
				at least $|S|/(nk)$ vectors from $S$ with any response.
					
				\item $|D_{\sigma_1}|> \dfrac{n-1}{n}\cdot|B_j|$:\\
				Consider the set of vectors in $D_{\sigma_1}$. By definition,
				all of them have $\sigma_1$ as their first entry. Their remaining
				$n-1$ entries are made up of the remaining $k-1$ colors, and no two of
				them coincide in all spots $2, 3, \ldots, n$ (otherwise they would be
				the same vector). Therefore, if we consider this new set of $n-1$
				vectors formed by taking spots $2, 3, \ldots, n$ of each vector in
				$D_{\sigma_1}$, they are a set of possible solutions for this
				Mastermind game with $n-1$ spots and $k-1$ colors. By the induction
				hypothesis, there is a guess of size $n-1$ that is guaranteed to
				eliminate $1/((n-1)(k-1))$ of these vectors. If we add $\sigma_1$
				to the
				beginning of this guess, it becomes a guess of size $n$. For any
				response $r$, any of the vectors in $D_{\sigma_1}$ will be eliminated
				if and only if their vector of spots $2, 3, \ldots, n$ was eliminated
				by the response $r-1$ in the smaller game of Mastermind. Therefore,
				this guess still guarantees that we eliminate
				\begin{align*}
				\frac{1}{(n-1)(k-1)}|D_{\sigma_1}|
				& \ge \frac{1}{(n-1)(k-1)}\cdot\frac{n-1}{n}\cdot|B_j|\\
				& \ge \frac{1}{(k-1)}\cdot\frac{1}{n}
				\cdot\frac{nk-1}{nk}\cdot|S|\\
				& \ge \frac{1}{nk-n}\cdot\frac{nk-1}{nk}\cdot|S|\\
				& \ge \frac{1}{nk}\cdot|S|
				\end{align*}
				\end{enumerate}
			
			\textit{Case 2.} If $j=0$, we consider the first component of all vectors
			in the bucket $B_0$. Since $j=0$ and $\sigma=\{\sigma_1, \sigma_2, \ldots,
			\sigma_n\}$, no vector in $B_0$ can contain $\sigma_1$ in the first
			position, so there are $k-1$ possible colors in the first position. By
			the pigeonhole principle, there must be one color which appears in the
			first spot of at least
				\begin{equation*}
				\frac{1}{k-1}\cdot|B_0|
				\end{equation*}
			vectors in $B_0$. If this color appears in $\sigma$, WLOG
			let the color be $\sigma_2$. Then in this case we consider the guess vector
			$\overline{\sigma}=\{\sigma_2, \sigma_1, \sigma_3, \ldots, \sigma_n\}$.
			Otherwise, call this new color $\tau$ and consider the guess vector
			$\overline{\sigma}=\{\tau, \sigma_2, \sigma_3, \ldots, \sigma_n\}$.
			In either case, $\overline{\sigma}$ matches these $|B_0|/(k-1)$ vectors
			in either one or two positions. Consider the
			buckets $C_i$ formed by the guess $\overline{\sigma}$. We know
				\begin{align*}
				|C_1|+|C_2| & \ge \frac{1}{k-1}\cdot |B_0|\\
				& \ge \frac{1}{k-1}\cdot\frac{nk-1}{nk}\cdot|S|\\
				& \ge \frac{1}{k-1}\cdot\frac{nk-n}{nk}\cdot|S|\\
				& = \frac{n}{nk}\cdot|S|\\
				& \ge \frac{2}{nk}\cdot|S|
				\end{align*}
			Therefore we have either
				\begin{equation*}
				|C_1|\ge\frac{1}{nk}\cdot|S|\qquad\text{or}
				\qquad|C_2|\ge\frac{1}{nk}\cdot|S|
				\end{equation*}
			Let this larger bucket be $C_{\alpha}$, then suppose
			\begin{equation*}|C_{\alpha}|\le\frac{nk-1}{nk}|S|\end{equation*}
			Then getting the response $\alpha$ will eliminate at least $|S|/(nk)$
			possible solutions from $S$, and getting any other response will
			eliminate at least $|C_\alpha|\ge |S|/(nk)$ possible solutions. Therefore
			the guess $\overline{\sigma}$ is guaranteed to eliminate at least
			$|S|/(nk)$ solutions from $S$.
			
			Otherwise we have
			\begin{equation*}|C_{\alpha}|>\frac{nk-1}{nk}|S|\end{equation*}
			Then, this satisfies the conditions for Case 1 above, where we have a
			guess whose largest bucket is $B_j$ with $j>0$.
			\end{enumerate}
		\end{proof}
	\item The minimax algorithm is a guessing strategy described by Knuth [DK76].
	At each turn, the algorithm assigns each of the possible guesses a score equal
	to the minimum number of solutions that they would eliminate over all
	possible responses from the codemaker. It then picks the guess with the maximum
	score.\\\\
	\textit{Corollary.} The minimax algorithm on Mastermind with $n$ spots, $k$
	colors, and no repeats takes at most
		\begin{equation*}
		\frac{\log\left(\frac{k!}{(k-n)!}\right)}{\log\left(\frac{nk}{nk-1}\right)}
		\end{equation*}
	turns to find the hidden vector.
		\begin{proof}
		Each step in the minimax algorithm cuts the set of possible solutions $S$ down
		by a factor of $(nk-1)/(nk)$ at each step. To complete the game, we need to
		cut $S$ down by
			\begin{equation*}
			\frac{1}{\left(\frac{k!}{(k-n)!}\right)}
			\end{equation*}
		Thus the required number of steps is
			\begin{equation*}
			\log_{\left(\frac{nk-1}{nk}\right)}\left(\frac{1}{\frac{k!}{(k-n)!}}\right)
			\end{equation*}
		Which is equivalent to the claim.
		\end{proof}
	This gives us the following asymptotic bounds:
		\begin{enumerate}[label=\roman*.]
		\item The minimax algorithm for Mastermind with $n$ spots, $k$
		colors, and no repeats takes at most $O(n^2k\log k)$ turns to find the
		hidden vector.
		
		\item In the same game where $n=k$, the minimax algorithm takes at most
		$O(n^3\log n)$ turns to find the hidden vector.
		\end{enumerate}
	Both of these asymptotic bounds are actually strict upper bounds, and 
	each of them differs from the exact bound by a constant factor which approaches
	1 as $n$ approaches infinity.
	\end{enumerate}
	
\clearpage
\section*{Extension of Previous Section to Repeated Colors}
	In order to replicate the bound of $nk$ with repeated colors, we need to make the
	following assumption:\footnote{The proof in the previous section can be extended
	easily to general $n$ and $k$: Substitute to show that there is always a guess that
	eliminates a factor of $1/n^2k$ possible solutions at each step. For a tighter
	bound with repeated colors, however, we need to make more restrictive assumptions.}
	There exists a guess vector $v$ such that all of the solutions in set $S$
	correspond to a single response $r$ after making the guess $v$. Note that this
	is always satisfied if the solutions are any set of solutions remaining after
	any set of guesses in an actual game of Mastermind. We simply let the guess vector
	$v$ be any guess vector that has already been guessed. Clearly, all solutions
	remaining correspond to whichever response was given to that guess vector, since
	guesses corresponding to any other response were eliminated. We will also prove the
	theorem for the special case of the very first guess separately.
	
	\begin{theorem}
	In a game of Mastermind with $n$ spots and $k$ colors, for any set of remaining
	solutions that can actually be achieved during a game of Mastermind,
	there is always a guess for which any response will eliminate at least
	$1/nk$ of the remaining solutions.
	\end{theorem}
	\begin{proof}
	We will follow the same logic as taken in the previous proof, but will describe
	only the parts that differ from the previous proof.
		\begin{enumerate}
		\item Because of the problem statement, we will separately prove the base
		case where $S$ is the set of all $k^n$ possible solutions before any guesses
		have been made. In this case, we guess the vector $\{1, 1, \ldots, 1\}$. The
		bucket $B_j$ has size
			\begin{equation*}
			|B_j| = \binom{n}{j}(k-1)^{n-j}
			\end{equation*}
		So the greatest fraction of possible solutions remaining is
			\begin{align*}
			\max_{j}\left(\frac{\binom{n}{j}(k-1)^{n-j}}{k^n}\right)
			\end{align*}
		When $k\ge n$, we have
			\begin{align*}
			\frac{\binom{n}{j}(k-1)^j}{k^n}
			& \le \frac{n^{n-j}(k-1)^j}{k^n}\\
			& \le \frac{k^{n-j}(k-1)^j}{k^n}\\
			& = \frac{(k-1)^j}{k^j}\\
			& \le \frac{k-1}{k}
			\end{align*}
		Therefore, we always eliminate a fraction at least $1/k\ge 1/nk$ of the
		solutions.
		
		When $k<n$, it is difficult to find the size of one bucket compared to the
		entire set, so instead we will compare one bucket to the sum of itself
		and the next bucket. This ratio will always be at least the ratio of
		one bucket compared to the entire set.
			\begin{align*}
			\frac{|B_{j+1}|}{|S|}
			& \le \frac{|B_{j+1}|}{|B_j|+|B_{j+1}|}\\
			& \le \frac{1}{\frac{|B_j|}{|B_{j+1}|}+1}\\
			& \le \frac{1}{\frac{\binom{n}{j}(k-1)^{n-j}}
			{\binom{n}{j+1}(k-1)^{n-j-1}}+1}\\
			& \le \frac{1}{\frac{j+1}{n-j}(k-1)+1}\\
			\end{align*}
			We plug in $k \ge 2$ (the entire problem is trivial when $k = 1$, as
			there is only 1 solution).
			\begin{align*}
			\ldots\quad & \le \frac{1}{\frac{j+1}{n-j}+1}\\
			& \le \frac{n-j}{n+1}\\
			& \le \frac{n}{n+1}\\
			& = 1-\frac{1}{n+1}\\
			& \le 1-\frac{1}{nk}
			\end{align*}
		Additionally, for bucket 0, we have:
		\begin{align*}
			\frac{|B_{0}|}{|S|}
			& = \frac{(k-1)^n}{k^n}\\
			& \le \frac{k-1}{k}\\
			& \le 1-\frac{1}{nk}
		\end{align*}
		So we have that the first guess of any game will always eliminate at least
		$1/nk$ of the remaining solutions.
		\item The argument for the base case where $n=1$ proceeds exactly as in the
		previous section.
		\item The induction hypothesis remains almost the same: Assume that for $n-1$
		spots and any number
		of colors $\widehat{k}$ (where $n\ge 2$), we are guaranteed that
		there is a guess that eliminates a fraction at least $1/(n\widehat{k})$
		of the solutions, given that there is a guess such that all solutions fall into
		the same bucket.

		\item\textit{Main Proof.}\\
		Let this bucket be $j$. From our assumptions $B_j = S$. Then we have two
		subcases as before: 

		\textit{Case 1.} If $j>0$, we make the following adjustments to the previous
		proof.
		\begin{enumerate}[label=\roman*.]
		\item For the first subcase, we are now allowed repeats, so we
		can always let
			\begin{equation*}
			\overline{\sigma}=\{\alpha, \sigma_2, \sigma_3, \ldots, \sigma_n\}
			\end{equation*}
		The proof of the rest of this subcase follows.
		
		\item For the second subcase, we proceed as the previous section did by
		considering $D_{\sigma_1}$, but now we are no longer guaranteed that the spots
		2 through $n$ of these vectors have no
		instance of $\sigma_1$
		since colors can be repeated. Whereas before, inducted on $n-1$ spots with
		$k-1$ possible colors, we now have to induct on $n-1$ spots still with $k$
		possible colors. This is fine,
		though, as by the induction hypothesis we can therefore eliminate
			\begin{align*}
			\frac{1}{(n-1)k}\cdot|D_{\sigma_1}|
			& \ge \frac{1}{(n-1)k}\cdot\frac{n-1}{n}\cdot|B_j|\\
			& = \frac{1}{nk}\cdot|S|
			\end{align*}
		It is important to check that the induction hypothesis holds- namely, that
		there exists a guess of size $n-1$ such that all of the $(n-1)$-vectors
		we're considering produce the same response.
		
		However, by our assumptions, $\sigma$ is an $n$-vector such that all of the
		$n$ vectors fall into a  single bucket $j$. Since every vector we're
		considering for induction is in the same sub-bucket 
		$D_{\sigma_1}$, if we consider only spots 2 through $n$, all of these
		$(n-1)$-vectors will have exactly one fewer hit than they did as $n$-vectors,
		(as all of them had a hit in the first spot), so all of them fall into the
		same bucket (bucket $j-1$) for the guess
		$\{\sigma_2, \sigma_3, \ldots, \sigma_n\}$, so the condition is satisfied.
		\end{enumerate}

		\textit{Case 2.} If $j=0$, we can no longer find a new $\overline{\sigma}$
		and switch to Case 1, so instead we
		induct similarly to Case 1. We split the
		bucket up into sub-buckets as before. By pigeonhole principle, there must exist
		an $\alpha$ such that
		$|D_\alpha| \ge |B_j|/(k-1) \ge |S|/(nk)$ (since $j=0$ we have no hits, so
		$|D_{\sigma_1}| = 0$).  We then 
		split into two subcases based on the size of $D_\alpha$ compared to the size of
		the entire bucket as we did in Case 1 and the proof proceeds in exactly the
		same manner.
		\end{enumerate}
	\end{proof}
	
\clearpage
\section*{Optimality of Mastermind for $(n, k) = (4, 6)$}
	Up to isomorphism, the first guess for this game will be one of the
	following:
		\begin{equation*}
		\{(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 1, 1), (0, 0, 1, 2), (0, 1, 2, 3)\}
		\end{equation*}
		Their respective worst-case responses and remaining solutions after that
		response are
		\begin{equation*}
		\{(0, 0)\rightarrow 625, (1, 0)\rightarrow 317, (0, 0)\rightarrow 256,
		(0, 1)\rightarrow 276, (0, 2)\rightarrow 312\}
		\end{equation*}
	For each of these guesses, there exists a response that leaves at least 256
	possible remaining solutions, as shown above. After this point, for every possible
	guess made, there will be at most fourteen possible responses given. Thus, at every
	step, the number of possible solutions remaining is reduced by at most a factor of
	fourteen. We use this information-theoretic lower bound to show that after the
	third guess, there will still be at least two solutions remaining:
		\begin{equation*}
		\left\lceil\frac{\left\lceil \frac{256}{14}\right\rceil}{14}\right\rceil = 2
		\end{equation*}
	Thus no strategy could guess the hidden vector on the fourth turn, so it is not
	possible for a deterministic guessing strategy to guarantee finding the
	hidden vector in fewer than five turns.

\clearpage
\section*{Lower Bound for Non-Adaptive Strategies Using Entropy}
\subsection*{Introduction to Entropy}
For a formal introduction to some of the properties of entropy, the authors found the
paper ``Three tutorial lectures on entropy and counting'' by David Galvin to be a
helpful resource. Here we will describe a few of the relevant properties of entropy
and provide informal justification.

Entropy is a metric for characterizing a random variable, which can be roughly
described as the expected amount of information yielded by that random variable when
it takes on one of the values in its domain. The entropy of a random variable $X$ with
domain $D$ is defined as
	\begin{equation*}
	H(X) = \sum_{x\in D}-\text{Pr}[X=x]\cdot\log_2\left(\text{Pr}[X=x]\right)
	\end{equation*}
Entropy has the following properties:
	\begin{enumerate}[label=]
	\item\underline{Property 1} If $X,Y$ uniquely determine each others'
	outcomes, then $H(X)=H(Y)$.
		\begin{proof}
		For every outcome $x_i\in D$, let $y_i$ be the outcome such that $\{X=x_i\}
		\Leftrightarrow \{Y=y_i\}$. Therefore,
		$\text{Pr}[X=x_i]=\text{Pr}[Y=y_i]$ for every pair of corresponding outcomes
		$x_i$ and $y_i$. Then we have
			\begin{align*}
			H(X) & = \sum_{i}-\text{Pr}[X=x_i]\cdot\log_2(\text{Pr}[X=x_i])\\
			& = \sum_{i}-\text{Pr}[Y=y_i]\cdot\log_2(\text{Pr}[Y=y_i])\\
			& = H(Y)
			\end{align*}
		\end{proof}
		
	\item\underline{Property 2} (Subadditivity) A vector $X=(X_1, X_2, \ldots, X_n)$
	of random variables (which is itself a random variable) has
		\begin{equation*}
		H(X)\le \sum_{i=1}^{n}H(X_i)
		\end{equation*}
	\textit{Justification.} To find the total information gained from $X$, we sum
	the information gained from each component. However, the components are not
	necessarily independent, therefore we might get some repeated information. The
	amount of \textit{unique} information we get from $X$ should therefore be
	at most the sum of the information gained from the individual components. For
	a formal proof, see pp. 8-9 in [DG14].
	\end{enumerate}

\subsection*{Proof}
\begin{theorem}
Every non-adaptive strategy for Mastermind with no repeats must submit at least
$O(n \log k)$ queries to uniquely identify all possible hidden vectors.
\end{theorem}

\begin{proof}
Our proof follows along similar lines as [Doerr et al]. Let $q_i$ be the $i$-th
guess of our deterministic strategy. Let $s$ be the smallest index such that all
$k!/(k-n)!$ codes are uniquely determined by the responses to $q_1, q_2, \ldots, q_s$.

Consider a code $Z$ sampled uniformly at random from the set of $k!/(k-n)!$ codes. Then
consider the random variables $Y_i = \textnormal{eq}(Z, q_i)$ be the response (\# black
hits) to guess $q_i$.
Then the vector $Y = (Y_1,Y_2,\ldots,Y_s)$ always uniquely determines, and is uniquely
determined by, $Z$. So by Property 1, $H(Z) = H(Y)$. Since $Z$ is a random
variable with $k!/(k-n)!$ outcomes of equal probability, we know
$H(Z) = \log_2(k!/(k-n)!)$.

By Property 2, we know
    \begin{equation*}
    H(Y) \leq \sum_{i=1}^s H(Y_i).
    \end{equation*}
So now we bound $H(Y_i)$. By its definition,
    \begin{equation}
    H(Y_i)=-\sum_{x=0}^n \textnormal{Pr}[Y_i=x]\cdot\log_2(\textnormal{Pr}[Y_{i}=x]).
    \end{equation}
What is Pr$[Y_i = x]$? This is the probability that $X$ is a solution vector with $x$
fixed points with respect to the query $q_i$. Using our previous terminology, this is
the probability that $X$ is in bucket $x$.
So we get\footnote{In going from the first to the second line,
we make the same combinatorial argument as before:
Choosing $x$ fixed points, choosing $n-x$ colors from the remaining $k-x$ possibilities
and permuting those $n-x$ positions overcounts all vectors with no repeated colors
and at least $x$ fixed points and thus overcounts the number of vectors with no
repeats and exactly $x$ fixed points (which is $|B_x|$).}
	\begin{align*}
	\textnormal{Pr}[Y_i = x] &= \frac{|B_x|}{\left(\frac{k!}{(k-n)!}\right)}\\
	&\leq \frac{\binom{n}{x}\frac{(k-x)!}{(k-n)!}}{\left(\frac{k!}{(k-n)!}\right)}\\
	& = \frac{1}{x!}\cdot
	\frac{\left(\frac{n!}{(n-x)!}\right)}{\left(\frac{k!}{(k-x)!}\right)}\\
	&= \frac{1}{x!}\cdot\frac{n(n-1)\cdots(n-x+1)}{k(k-1)\cdots(k-x+1)}\\
	&\leq \frac{1}{x!}
	\end{align*}
\clearpage
We want to plug this upper bound into equation 1.
For $\alpha<1/e$, $f(\alpha)=-\alpha\log_2\alpha$ is an increasing function.
So we can plug in the upper bound of $\text{Pr}[X=x]\le 1/x!$ when $x\ge 3$ and
still have an upper bound. For the first three values, we will use the upper bound
$f(\alpha)\le 1/(e\log 2)$. 
    \begin{align*}
    H(Y_i) &\leq \frac{3}{e\log 2}+\sum_{x=3}^n-\frac{1}{x!}\cdot\log_2(\frac{1}{x!})\\
    &= \frac{3}{e\log 2}+\sum_{x=3}^n \frac{\log_2(x!)}{x!}\\
    &\leq \frac{3}{e\log 2}+\sum_{x=3}^n \frac{x \log_2 x}{x!}\\
    &\leq \frac{3}{e\log 2}+\sum_{x=3}^n \frac{x (x-1)}{x!}\\
    &= \frac{3}{e\log 2}+\sum_{x=3}^n \frac{1}{(x-2)!}\\
    &\leq \frac{3}{e\log 2}+e-1\\
    & < 4
    \end{align*}
Combining this with the summation above gives $H(Y) \leq 4s$, so we get
	\begin{align*}
	H(X) &\leq H(Y)\\
	\log_2\left(\frac{k!}{(k-n)!}\right) &\leq 4s\\
	s &\geq \frac{1}{4}\log_2\left(\frac{k!}{(k-n)!}\right)
	\end{align*}

This gives us a lower bound of $O(n \log k)$ turns for any non-adaptive strategy for
Mastermind with no repeats.
\end{proof}

\clearpage

\section*{Upper Bound on Non-Adaptive Strategies for Mastermind}
For a game of mastermind with $n$ spots and $k$ colors (with or without repeats), every
hidden vector and query can be represented by a matrix $A\in\mathbb{R}^{n\times k}$,
defined by $A_{ij}=1$ if the $i$-th peg is the $j$-th color, and 0 otherwise.
In that way, for a given hidden vector matrix $X$ and query matrix $Q$,
the codemaker's response to the query is given by $X\cdot Q$.
Let $S$ be the space which the set of possible queries spans. Then, for any
$Q_1, \ldots, Q_k$ which span $S$, any matrix $X\in S$, including all hidden
permutations, can be uniquely determined by the numbers $X\cdot Q_i$ for $i\in [1,k]$.

As the set of all possible queries spans $S$, there must exist a subset of queries,
$K$, with $|K|=\text{dim}(S)$ and $K$ spanning $S$. Thus, the queries of $K$ uniquely
determine any hidden permutation. $S\subset \mathbb{R}^{n\times k}$,
so $\text{dim}(S)\le \text{dim}(\mathbb{R}^{n\times k})=n\cdot k$.

Thus, there exists a set of at most $n\cdot k$ queries which uniquely determines every
possible hidden permutation.

\clearpage
\section*{Existence of a Deterministic $O(k\log k)$ Non-Adaptive Strategy}
This proof was inspired by [GK00].
	\begin{theorem}
    When $k\ge n$, there always exists a set of $O(k\log k)$ queries,
    the responses to which will distinguish between all possible $n$-vectors from an
    alphabet of size $k$.
	\end{theorem}

	\begin{proof}
	To prove the theorem, we select a set of $4k\log k$ random guesses and show that the with probability $<1$
	there exists a pair of vectors that are indistinguishable by these guesses. This would imply
	the existance of a set of guesses that distinguishes between every pair of possible solutions.
	As in [GK00], we only look at distinguishing between ``critical pairs'', which are
	two vectors in which we only analyze the spots that differ. For example, the
	following vectors produce the following critical pair:\\
		\begin{center}
		\includegraphics[width=0.7\textwidth, keepaspectratio=true]{criticalpair.pdf}
		\end{center}
	The theorem follows directly if we can show that there's a non-zero probability
	that $O(k\log k)$ queries distinguish between every possible critical pair. We
	use the following lemma:
    	\begin{lemma}
    	Given a query $q$ and a critical pair $v_1,v_2$ of size $x$,
		the probability that $v_1$ and $v_2$ receive the same response is at most
		$(1-1/k)^x$. That is,
    	\begin{equation*}
    	\sum_{i=0}^{\left\lfloor x/2 \right\rfloor}
		\textnormal{Pr}[\textnormal{Eq}(q,v_1) = i
    	\cap\textnormal{Eq}(q,v_2) = i] \leq \left(1-\frac{1}{k}\right)^x
    	\end{equation*}
    	\end{lemma}		
		
		\begin{proof}
		We prove two separate cases:
			\begin{enumerate}[label=]
			\item\textit{Case 1.} When $x \leq k-3$, we have
            	\begin{align*}
            	\text{Pr}[\text{Eq}(q,v_1) = i & \cap \text{Eq}(q,v_2) = i]\\\\
            	&= \binom{x}{i}\binom{x-i}{i}\left(\frac{1}{k}\right)^{2i}
				\left(\frac{k-2}{k}\right)^{x-2i}\\\\
            	&=\frac{x!(k-2)^{x-2i}}{(i!)^2(k^x)(x-2i)!} \\\\
            	&\leq \frac{x^{2i}}{(k-2)^{2i}(i!)}\left(\frac{k-2}{k}\right)^x.
            	\end{align*}
            Now we have
                \begin{align*}
                \sum_{i=0}^{\left\lfloor x/2 \right\rfloor}
                \text{Pr}[\text{Eq}(q,v_1) = i
                & \cap \text{Eq}(q,v_2) = i] \left(\frac{k}{k-1}\right)^x\\
                &\leq \sum_{i=0}^{\left\lfloor x/2 \right\rfloor}
                \frac{x^{2i}}{(k-2)^{2i}(i!)}\left(\frac{k-2}{k}\right)^x
                \left(\frac{k}{k-1}\right)^x \\\\
                &=\sum_{i=0}^{\left\lfloor x/2 \right\rfloor}
                \frac{x^{2i}}{(k-2)^{2i}(i!)}
                \left(1-\frac{1}{k-1}\right)^x \\\\
                &<\left(1-\frac{1}{k-1}\right)^x ~ \sum_{i=0}^{\infty}
                \left(\frac{x^2}{(k-2)^2}\right)^i\left(\frac{1}{i!}\right) \\
                &<e^{-\frac{x}{k-1}}\cdot e^\frac{x^2}{(k-2)^2} \\\\
                &<1\quad \text{ for } x \leq (k-3).
                \end{align*}
            Multiplying both sides of this inequality by $(1-1/k)^x$ results in
            the claim.\\
            
            \item\textit{Case 2.} When $k-2 \leq x \leq k$, we manipulate the sum
            slightly differently: First we multiply both sides of the inequality by
            $(k-2)^x/k^x$ to give,
            	\begin{equation*}
				\sum_{i=0}^{\lfloor x/2 \rfloor} \binom{x}{i}\binom{x-i}{i}(k-2)^{-2i}
				\le \left(\frac{k-1}{k-2}\right)^x
				\end{equation*}
			We will bound the LHS from above by a constant:
                \begin{equation*}
                \sum_{i=0}^{\lfloor x/2 \rfloor} \binom{x}{i}\binom{x-i}{i}(k-2)^{-2i}
                =\sum_{i=0}^{\lfloor x/2 \rfloor} \frac{\frac{x(x-1)
                \cdots(x-2i+1)}{(k-2)^{2i}}}{i!^2}
                \end{equation*}
            When $i\ge 3$,
            	\begin{align*}
				x(x-1)\cdots(x-2i+1) & \le k(k-1)\cdots(k-2i+1)\\
	            & \le(k-2)^{2i}
	            \end{align*}
            Thus, 
                \begin{align*}
                \sum_{i=3}^{\lfloor x/2 \rfloor} \binom{x}{i}\binom{x-i}{i}(k-2)^{-2i}
                & \le \sum_{i=3}^{\lfloor x/2 \rfloor} \frac{1}{(i!)^2}\\
                & \le \sum_{i=3}^\infty \frac{1}{(i!)^2}\\
                & < .0296.
                \end{align*}
            We separately bound the contribution of the terms for $i=0, 1, 2$:
                \begin{align*}
                \sum_{i=0}^{2} \binom{x}{i}\binom{x-i}{i}(k-2)^{-2i}
                & = 1+\frac{x(x-1)}{(k-2)^2}+\frac{x(x-1)(x-2)(x-3)}{4(k-2)^4} \\
                & \le 1+\frac{k(k-1)}{(k-2)^2}+\frac{k(k-1)(k-2)(k-3)}{4(k-2)^4} \\
                & < 2.5536 \text{ for } k\ge 14.
                \end{align*}
            Combining the sub-cases when $0\le i< 3$ and $i\ge 3$ gives
            	\begin{align*}
            	\sum_{i=0}^{\lfloor x/2 \rfloor} \binom{x}{i}\binom{x-i}{i}(k-2)^{-2i}
            	& < 2.5832 \\
            	& < \left(1+\frac{1}{k-2}\right)^{k-2} \text{ for } k\ge 14\\
            	& = \left(\frac{k-1}{k-2}\right)^{x}.
            	\end{align*}
			\end{enumerate}
		Cases 1 and 2 together show that for all critical pairs $v_1$, $v_2$ with
		size $x$, the probability that $v_1$ and $v_2$ receive the same response
		with respect to a given query is at most $(1-1/k)^x$.
		\end{proof}
    
    Using Lemma 1, we get 
	\begin{equation*}
	\text{Pr}[v_1,v_2 \text{ not distinguished by $s$ random queries}] \leq \left(1-\frac{1}{k}\right)^{xs}
	\end{equation*}
Now we want to calculate
    \begin{align*}
	\text{Pr}[\exists\text{ pair } v_1, v_2 & \text{ not distinguished}]\\
	& = \text{Pr}\left[\bigcup_{v_1,v_2 \text{ critical pair}} \textnormal{Eq}(q,v_1)=    	\textnormal{Eq}(q,v_2)\right] \\
	& \le \sum_{v_1,v_2 \text{ critical pair}} \textnormal{Pr}[\textnormal{Eq}(q,v_1)=
	\textnormal{Eq}(q,v_2)] 
	\end{align*}
By Boole's Inequality.

But for a fixed length $x$, this probability is the same for all critical pairs
$(v_1,v_2)$ of length $x$. This allows us to rewrite the above sum as
	\begin{equation*}
    \sum_{x=1}^n(\text{\# of critical pairs of size }x)
	\cdot\text{Pr}[\text{single pair of lenteh $x$ not distinguished}].
	\end{equation*}
So we now use $s = 4k \log k$ queries to bound
	\begin{align*}
	\sum_{x=1}^n&(\text{\# of critical pairs of size }x)
	\cdot\text{Pr}[\text{single pair of lenteh $x$ not distinguished}]\\
	& = \sum_{x=1}^n \left[\binom{n}{x}k^x(k-1)^x
    \cdot\left(\sum_{i=0}^{\left\lfloor x/2 \right\rfloor}
	\textnormal{Pr}[\textnormal{Eq}(q,v_1) = i
    \cap\textnormal{Eq}(q,v_2) = i]\right)^s ~\right]\\
    &\le \sum_{x=1}^n k^{3x} \left(1-\frac{1}{k}\right)^{(4k\log k)x} \\
    &< \sum_{x=1}^n k^{3x} \left(\frac{1}{e}\right)^{(4\log k)x} \\
    &= \sum_{x=1}^n k^{3x} \left(\frac{1}{k}\right)^{4 x} \\
    &\le \sum_{x=1}^n \frac{1}{k} \\
    &\le 1.
    \end{align*}
So we have $\text{Pr}[\exists\text{ pair } v_1, v_2  \text{ not distinguished}] < 1$.
Thus, for $k\ge n$ there always exists a set of $O(k\log k)$ queries that
distinguishes between all possible $n$-vectors from an alphabet of size $k$.
\end{proof}

\clearpage
\section*{Lower Bound for non-adaptive strategies}
\begin{theorem}
Any non-adaptive strategy must make at least $(k-1)/2$ guesses to distinguish all
possible hidden vectors.
\end{theorem}
\begin{proof}
Assume for the sake of contradiction that in some strategy that distinguishes all
possible hidden vectors, there exists a pair of colors such that neither color was
guessed in the first two spots (WLOG let these colors be 1 and 2). Then it would be
impossible to distinguish between the hidden vectors $(1,2,\ldots,n)$ and
$(2,1,\ldots,n)$, which contradicts the assumptions. Therefore, for any winning
strategy, at least $k-1$ colors were guessed in these two spots. Since we can only
guess two colors in those spots per turn, this gives us the lower bound of
$(k-1)/2 =O(k)$ turns necessary to beat the game.
\end{proof}
\end{document}





























